{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subramaniya-pillai/data_engineering/blob/main/PySpark_Master_Task_Set.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPDt55xI-9uN"
      },
      "source": [
        "# PySpark Master Task Set\n"
      ],
      "id": "gPDt55xI-9uN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNOGj_EG-9uP"
      },
      "source": [
        "## Task 1: Data Ingestion & Exploration"
      ],
      "id": "QNOGj_EG-9uP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVkU-mi8-9uQ"
      },
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"PySparkMasterTaskSet\").getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "XVkU-mi8-9uQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pe8ov6V-9uQ",
        "outputId": "cbe82983-b7a7-4588-d7f5-e334a17111c8"
      },
      "source": [
        "\n",
        "# Load the datasets\n",
        "customers_df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "orders_df = spark.read.csv(\"/content/orders (2).csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show inferred schema for both datasets\n",
        "customers_df.printSchema()\n",
        "orders_df.printSchema()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Email: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- SignupDate: date (nullable = true)\n",
            "\n",
            "root\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- OrderDate: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "id": "_Pe8ov6V-9uQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqYD65zi-9uR",
        "outputId": "713d7e1a-cd86-4cc0-bdb4-b94d45312172"
      },
      "source": [
        "\n",
        "# Count total number of customers and orders\n",
        "num_customers = customers_df.count()\n",
        "num_orders = orders_df.count()\n",
        "\n",
        "print(f\"Total number of customers: {num_customers}\")\n",
        "print(f\"Total number of orders: {num_orders}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of customers: 5\n",
            "Total number of orders: 7\n"
          ]
        }
      ],
      "id": "YqYD65zi-9uR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6sMWays-9uR",
        "outputId": "5f417321-d513-459c-852b-2c60f124ef08"
      },
      "source": [
        "\n",
        "# Show distinct cities\n",
        "customers_df.select(\"City\").distinct().show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|     City|\n",
            "+---------+\n",
            "|Bangalore|\n",
            "|  Chennai|\n",
            "|   Mumbai|\n",
            "|    Delhi|\n",
            "|Hyderabad|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "id": "O6sMWays-9uR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzoJ8xJq-9uR"
      },
      "source": [
        "## Task 2: DataFrame Transformations"
      ],
      "id": "fzoJ8xJq-9uR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS4h8OXV-9uR",
        "outputId": "b9e85a99-624e-437e-a30f-754d4ec96f97"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Add a column TotalAmount = Price * Quantity\n",
        "orders_df = orders_df.withColumn(\"TotalAmount\", col(\"Price\") * col(\"Quantity\"))\n",
        "\n",
        "# Create a new column OrderYear from OrderDate\n",
        "orders_df = orders_df.withColumn(\"OrderYear\", year(col(\"OrderDate\")))\n",
        "\n",
        "# Filter orders with TotalAmount > 10000\n",
        "orders_df.filter(col(\"TotalAmount\") > 10000).show()\n",
        "\n",
        "# Drop the Email column from customers\n",
        "customers_df = customers_df.drop(\"Email\")\n",
        "customers_df.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|OrderID|CustomerID|Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|      1|       101| Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|\n",
            "|      3|       102| Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|\n",
            "|      7|       102|  Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "\n",
            "+----------+-----+---------+----------+\n",
            "|CustomerID| Name|     City|SignupDate|\n",
            "+----------+-----+---------+----------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|\n",
            "|       102| Neha|    Delhi|2023-01-15|\n",
            "|       103| Ravi|Bangalore|2021-11-01|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|\n",
            "|       105| Amit|  Chennai|2023-03-10|\n",
            "+----------+-----+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "id": "nS4h8OXV-9uR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ifq3oO-9uS"
      },
      "source": [
        "## Task 3: Handling Nulls & Conditionals"
      ],
      "id": "93ifq3oO-9uS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0_-0eDN-9uS"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import when, lit, to_date\n",
        "\n",
        "# Simulate a null in City (set City of one row to null)\n",
        "customers_df = customers_df.withColumn(\"City\", when(col(\"CustomerID\") == 105, None).otherwise(col(\"City\")))\n",
        "\n",
        "# Fill null City with \"Unknown\"\n",
        "customers_df = customers_df.fillna({\"City\": \"Unknown\"})\n",
        "\n",
        "# Label customers as “Loyal” if SignupDate is before 2022, else “New”\n",
        "customers_df = customers_df.withColumn(\"CustomerType\", when(col(\"SignupDate\") < \"2022-01-01\", \"Loyal\").otherwise(\"New\"))\n",
        "\n",
        "# Create OrderType column: \"Low\" if < 5000, \"High\" if ≥ 5000\n",
        "orders_df = orders_df.withColumn(\"OrderType\", when(col(\"TotalAmount\") < 5000, \"Low\").otherwise(\"High\"))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "x0_-0eDN-9uS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPDz-LNx-9uS"
      },
      "source": [
        "## Task 4: Joins & Aggregations"
      ],
      "id": "sPDz-LNx-9uS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRIvStQq-9uS",
        "outputId": "f326c949-346e-4c83-f91e-dcc415aa4256"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import sum as _sum, count as _count, desc\n",
        "\n",
        "# Join customers and orders on CustomerID\n",
        "joined_df = customers_df.join(orders_df, \"CustomerID\")\n",
        "\n",
        "# Get total orders and revenue per city\n",
        "joined_df.groupBy(\"City\").agg(_count(\"*\").alias(\"TotalOrders\"), _sum(\"TotalAmount\").alias(\"TotalRevenue\")).show()\n",
        "\n",
        "# Show top 3 customers by total spend\n",
        "joined_df.groupBy(\"CustomerID\", \"Name\").agg(_sum(\"TotalAmount\").alias(\"TotalSpent\")).orderBy(desc(\"TotalSpent\")).show(3)\n",
        "\n",
        "# Count how many products each category has sold\n",
        "joined_df.groupBy(\"Category\").agg(_sum(\"Quantity\").alias(\"TotalSold\")).show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+------------+\n",
            "|     City|TotalOrders|TotalRevenue|\n",
            "+---------+-----------+------------+\n",
            "|Bangalore|          1|      3500.0|\n",
            "|   Mumbai|          2|    101200.0|\n",
            "|  Unknown|          1|      2500.0|\n",
            "|    Delhi|          2|     50000.0|\n",
            "|Hyderabad|          1|      5000.0|\n",
            "+---------+-----------+------------+\n",
            "\n",
            "+----------+-----+----------+\n",
            "|CustomerID| Name|TotalSpent|\n",
            "+----------+-----+----------+\n",
            "|       101|  Ali|  101200.0|\n",
            "|       102| Neha|   50000.0|\n",
            "|       104|Sneha|    5000.0|\n",
            "+----------+-----+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----------+---------+\n",
            "|   Category|TotalSold|\n",
            "+-----------+---------+\n",
            "| Stationery|        5|\n",
            "|Electronics|        5|\n",
            "|  Furniture|        1|\n",
            "| Appliances|        1|\n",
            "+-----------+---------+\n",
            "\n"
          ]
        }
      ],
      "id": "xRIvStQq-9uS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJwR7swp-9uS"
      },
      "source": [
        "## Task 5: Spark SQL Tasks"
      ],
      "id": "sJwR7swp-9uS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP6wHMX4-9uS"
      },
      "source": [
        "\n",
        "# Create database and switch to it\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS sales\")\n",
        "spark.catalog.setCurrentDatabase(\"sales\")\n",
        "\n",
        "# Save datasets as tables\n",
        "customers_df.write.mode(\"overwrite\").saveAsTable(\"customers\")\n",
        "orders_df.write.mode(\"overwrite\").saveAsTable(\"orders\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "mP6wHMX4-9uS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx4OypfH-9uS",
        "outputId": "77dd9819-213a-4b1f-841e-e38310b3e887"
      },
      "source": [
        "\n",
        "# SQL Queries\n",
        "spark.sql(\"\"\"\n",
        "SELECT o.* FROM orders o\n",
        "JOIN customers c ON o.CustomerID = c.CustomerID\n",
        "WHERE c.City = 'Delhi'\n",
        "\"\"\" ).show()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT Category, AVG(TotalAmount) AS AvgOrderValue\n",
        "FROM orders\n",
        "GROUP BY Category\n",
        "\"\"\" ).show()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE VIEW monthly_orders AS\n",
        "SELECT MONTH(OrderDate) AS Month, SUM(TotalAmount) AS MonthlyTotal\n",
        "FROM orders\n",
        "GROUP BY MONTH(OrderDate)\n",
        "\"\"\" )\n",
        "\n",
        "spark.sql(\"SELECT * FROM monthly_orders\").show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|OrderID|CustomerID|Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|      3|       102| Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High|\n",
            "|      7|       102|  Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|     High|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|   Category|AvgOrderValue|\n",
            "+-----------+-------------+\n",
            "| Stationery|       2500.0|\n",
            "|Electronics|      37800.0|\n",
            "|  Furniture|       3500.0|\n",
            "| Appliances|       5000.0|\n",
            "+-----------+-------------+\n",
            "\n",
            "+-----+------------+\n",
            "|Month|MonthlyTotal|\n",
            "+-----+------------+\n",
            "|    1|    101200.0|\n",
            "|    3|     32500.0|\n",
            "|    2|     28500.0|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "id": "Jx4OypfH-9uS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQGHc9X1-9uT"
      },
      "source": [
        "## Task 6: String & Date Functions"
      ],
      "id": "tQGHc9X1-9uT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQfnaoZb-9uT"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import regexp_replace, concat_ws, datediff, current_date, month\n",
        "\n",
        "# Mask emails using regex (for original email view, reload customers.csv)\n",
        "customers_raw = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "customers_masked = customers_raw.withColumn(\"MaskedEmail\", regexp_replace(\"Email\", r\"(^.).*(@.*)\", r\"\\1***\\2\"))\n",
        "\n",
        "# Concatenate Name and City as “Name from City”\n",
        "customers_df = customers_df.withColumn(\"NameCity\", concat_ws(\" from \", \"Name\", \"City\"))\n",
        "\n",
        "# Use datediff() to calculate customer age in days since SignupDate\n",
        "customers_df = customers_df.withColumn(\"AgeInDays\", datediff(current_date(), col(\"SignupDate\")))\n",
        "\n",
        "# Extract month name from OrderDate\n",
        "from pyspark.sql.functions import date_format\n",
        "orders_df = orders_df.withColumn(\"MonthName\", date_format(\"OrderDate\", \"MMMM\"))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "DQfnaoZb-9uT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwsRa3DC-9uT"
      },
      "source": [
        "## Task 7: UDFs and Complex Logic"
      ],
      "id": "XwsRa3DC-9uT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDC7-qNd-9uT",
        "outputId": "ab21796d-3c5a-4908-fa2e-7ddcc57ae9a9"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# UDF to tag customers\n",
        "def customer_tag(spend):\n",
        "    if spend > 50000:\n",
        "        return \"Gold\"\n",
        "    elif spend >= 10000:\n",
        "        return \"Silver\"\n",
        "    else:\n",
        "        return \"Bronze\"\n",
        "\n",
        "tag_udf = udf(customer_tag, StringType())\n",
        "\n",
        "# Apply UDF to total spend per customer\n",
        "spend_df = joined_df.groupBy(\"CustomerID\").agg(_sum(\"TotalAmount\").alias(\"TotalSpent\"))\n",
        "spend_df = spend_df.withColumn(\"CustomerTag\", tag_udf(\"TotalSpent\"))\n",
        "spend_df.show()\n",
        "\n",
        "# UDF to shorten product names\n",
        "shorten_udf = udf(lambda name: name[:3] + \"...\" if len(name) > 3 else name, StringType())\n",
        "orders_df = orders_df.withColumn(\"ShortProduct\", shorten_udf(\"Product\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------+\n",
            "|CustomerID|TotalSpent|CustomerTag|\n",
            "+----------+----------+-----------+\n",
            "|       101|  101200.0|       Gold|\n",
            "|       103|    3500.0|     Bronze|\n",
            "|       102|   50000.0|     Silver|\n",
            "|       105|    2500.0|     Bronze|\n",
            "|       104|    5000.0|     Bronze|\n",
            "+----------+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "id": "sDC7-qNd-9uT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Gf5hGk-9uT"
      },
      "source": [
        "## Task 8: Parquet & Views"
      ],
      "id": "L-Gf5hGk-9uT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibwGWehQ-9uT",
        "outputId": "51a15ed1-d4c5-42fa-a0a6-44b9ef003377"
      },
      "source": [
        "\n",
        "# Save joined result as Parquet\n",
        "joined_df.write.mode(\"overwrite\").parquet(\"joined_data.parquet\")\n",
        "\n",
        "# Read back Parquet file and verify schema\n",
        "parquet_df = spark.read.parquet(\"joined_data.parquet\")\n",
        "parquet_df.printSchema()\n",
        "\n",
        "# Create and query a global temp view\n",
        "parquet_df.createOrReplaceGlobalTempView(\"global_orders\")\n",
        "spark.sql(\"SELECT * FROM global_temp.global_orders LIMIT 5\").show()\n",
        "\n",
        "# Compare performance: CSV read vs Parquet read (basic timing comparison)\n",
        "import time\n",
        "\n",
        "start_csv = time.time()\n",
        "spark.read.csv(\"/content/orders (2).csv\", header=True, inferSchema=True).count()\n",
        "end_csv = time.time()\n",
        "\n",
        "start_parquet = time.time()\n",
        "spark.read.parquet(\"joined_data.parquet\").count()\n",
        "end_parquet = time.time()\n",
        "\n",
        "print(f\"CSV Read Time: {end_csv - start_csv:.2f}s\")\n",
        "print(f\"Parquet Read Time: {end_parquet - start_parquet:.2f}s\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- SignupDate: date (nullable = true)\n",
            " |-- CustomerType: string (nullable = true)\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- OrderDate: date (nullable = true)\n",
            " |-- TotalAmount: double (nullable = true)\n",
            " |-- OrderYear: integer (nullable = true)\n",
            " |-- OrderType: string (nullable = true)\n",
            "\n",
            "+----------+-----+---------+----------+------------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|CustomerID| Name|     City|SignupDate|CustomerType|OrderID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType|\n",
            "+----------+-----+---------+----------+------------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|         New|      1|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|\n",
            "|       101|  Ali|   Mumbai|2022-05-10|         New|      2|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|\n",
            "|       102| Neha|    Delhi|2023-01-15|         New|      3|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High|\n",
            "|       103| Ravi|Bangalore|2021-11-01|       Loyal|      4|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|       Loyal|      5|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High|\n",
            "+----------+-----+---------+----------+------------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "\n",
            "CSV Read Time: 1.08s\n",
            "Parquet Read Time: 0.65s\n"
          ]
        }
      ],
      "id": "ibwGWehQ-9uT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPHaqcqC-9uT",
        "outputId": "ca6465ff-7302-4032-d988-b61952f1331c"
      },
      "source": [
        "\n",
        "# UDF to shorten product names (first 3 letters + ...)\n",
        "shorten_udf = udf(lambda name: name[:3] + \"...\" if name else \"\", StringType())\n",
        "orders_df = orders_df.withColumn(\"ShortProduct\", shorten_udf(\"Product\"))\n",
        "orders_df.select(\"Product\", \"ShortProduct\").show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|  Product|ShortProduct|\n",
            "+---------+------------+\n",
            "|   Laptop|      Lap...|\n",
            "|    Mouse|      Mou...|\n",
            "|   Tablet|      Tab...|\n",
            "|Bookshelf|      Boo...|\n",
            "|    Mixer|      Mix...|\n",
            "| Notebook|      Not...|\n",
            "|    Phone|      Pho...|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ],
      "id": "PPHaqcqC-9uT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDSRB0tL-9uT"
      },
      "source": [
        "## Task 8: Parquet & Views"
      ],
      "id": "DDSRB0tL-9uT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FInvgZA-9uT",
        "outputId": "a3241ab8-f601-44f2-d0f7-2ec0aea99896"
      },
      "source": [
        "\n",
        "# Save the joined result as a Parquet file\n",
        "parquet_path = \"/tmp/joined_data.parquet\"\n",
        "joined_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
        "\n",
        "# Read it back and verify schema\n",
        "parquet_df = spark.read.parquet(parquet_path)\n",
        "parquet_df.printSchema()\n",
        "parquet_df.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- SignupDate: date (nullable = true)\n",
            " |-- CustomerType: string (nullable = true)\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- OrderDate: date (nullable = true)\n",
            " |-- TotalAmount: double (nullable = true)\n",
            " |-- OrderYear: integer (nullable = true)\n",
            " |-- OrderType: string (nullable = true)\n",
            "\n",
            "+----------+-----+---------+----------+------------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|CustomerID| Name|     City|SignupDate|CustomerType|OrderID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType|\n",
            "+----------+-----+---------+----------+------------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|         New|      1|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|\n",
            "|       101|  Ali|   Mumbai|2022-05-10|         New|      2|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|\n",
            "|       102| Neha|    Delhi|2023-01-15|         New|      3|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High|\n",
            "|       103| Ravi|Bangalore|2021-11-01|       Loyal|      4|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|       Loyal|      5|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High|\n",
            "|       105| Amit|  Unknown|2023-03-10|         New|      6| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|      Low|\n",
            "|       102| Neha|    Delhi|2023-01-15|         New|      7|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|     High|\n",
            "+----------+-----+---------+----------+------------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "\n"
          ]
        }
      ],
      "id": "_FInvgZA-9uT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsieqah_-9uT",
        "outputId": "e77924cf-8022-4387-d37f-a50fd4dc0ac9"
      },
      "source": [
        "\n",
        "# Create and query a global temp view\n",
        "parquet_df.createOrReplaceGlobalTempView(\"global_joined_view\")\n",
        "spark.sql(\"SELECT City, COUNT(*) as Orders FROM global_temp.global_joined_view GROUP BY City\").show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|     City|Orders|\n",
            "+---------+------+\n",
            "|Bangalore|     1|\n",
            "|   Mumbai|     2|\n",
            "|  Unknown|     1|\n",
            "|    Delhi|     2|\n",
            "|Hyderabad|     1|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ],
      "id": "zsieqah_-9uT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNm66xh3-9uU",
        "outputId": "abd9b482-160d-48ac-b836-da91f2bd0bca"
      },
      "source": [
        "\n",
        "# Compare performance between CSV read and Parquet read (example via timing)\n",
        "import time\n",
        "\n",
        "start_csv = time.time()\n",
        "csv_test = spark.read.csv(\"/content/orders (2).csv\", header=True, inferSchema=True).count()\n",
        "end_csv = time.time()\n",
        "\n",
        "start_parquet = time.time()\n",
        "parquet_test = spark.read.parquet(parquet_path).count()\n",
        "end_parquet = time.time()\n",
        "\n",
        "print(f\"CSV read time: {end_csv - start_csv:.4f} seconds\")\n",
        "print(f\"Parquet read time: {end_parquet - start_parquet:.4f} seconds\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV read time: 0.8908 seconds\n",
            "Parquet read time: 0.4739 seconds\n"
          ]
        }
      ],
      "id": "tNm66xh3-9uU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}